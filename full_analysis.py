#!/usr/bin/env python3
"""
Memory Controller Analysis Tool

This script analyzes timing measurement data from memory access patterns to identify
DRAM bank conflict patterns and row buffer behavior. It uses Galois field (GF(2))
linear algebra to find bit masks that predict memory access conflicts.

Author: [Author name]
Date: July 2025
"""

import argparse
import numpy as np
import pandas as pd
import galois
from collections import defaultdict, Counter
from itertools import combinations
import matplotlib.pyplot as plt
import itertools
import sys
from datetime import datetime

def print_header(title, char='=', width=80):
    """Print a formatted header for output sections."""
    print(f"\n{char * width}")
    print(f"  {title}")
    print(f"{char * width}")

def print_section(title, char='-', width=60):
    """Print a formatted section header."""
    print(f"\n{char * width}")
    print(f"  {title}")
    print(f"{char * width}")

def print_timestamp():
    """Print current timestamp."""
    print(f"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

def parity(arr):
    """Calculate parity (XOR reduction) of bit arrays."""
    return (np.bitwise_xor.reduce(arr, axis=1, dtype=np.uint8)
            if arr.size else
            np.zeros(arr.shape[0], np.uint8))

# ════════════════════════════════════════════════════════════════════════════════
# Command Line Interface
# ════════════════════════════════════════════════════════════════════════════════

def parse_arguments():
    """Parse and validate command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Analyze memory timing data to identify DRAM bank conflict patterns",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s data.csv --thresh 150
  %(prog)s data.csv --thresh 150 --subsample 2000 --repeat 100
  %(prog)s data.csv --thresh 150 --limit 100000 --sensitivity 0.1
        """
    )
    
    parser.add_argument('csv', 
                       help='Path to CSV input file generated by the data collection tool')
    parser.add_argument('--thresh', type=int, required=True,
                       help='Threshold latency (cycles) separating conflicts from non-conflicts')
    parser.add_argument('--subsample', type=int, default=1000,
                       help='Subsample size for nullspace analysis (default: 1000)')
    parser.add_argument('--repeat', type=int, default=50,
                       help='Number of repetitions for subsampling (default: 50)')
    parser.add_argument('--sensitivity', type=float, default=0.05,
                       help='Sensitivity threshold for row bit analysis (default: 0.05)')
    parser.add_argument('--limit', type=int, default=None,
                       help='Limit the number of pairs to process (for testing)')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose output')
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.thresh <= 0:
        parser.error("Threshold must be positive")
    if args.subsample <= 0:
        parser.error("Subsample size must be positive")
    if args.repeat <= 0:
        parser.error("Repeat count must be positive")
    if not (0 < args.sensitivity < 1):
        parser.error("Sensitivity must be between 0 and 1")
    
    return args

args = parse_arguments()

CSV = args.csv
THRESH = args.thresh
SUBSAMPLE = args.subsample
REPEAT = args.repeat
SENSITIVITY = args.sensitivity
LIMIT = args.limit
VERBOSE = args.verbose

# ════════════════════════════════════════════════════════════════════════════════
# Data Loading and Preprocessing
# ════════════════════════════════════════════════════════════════════════════════

def load_and_preprocess_data():
    """Load CSV data and perform initial preprocessing."""
    print_header("MEMORY TIMING DATA ANALYSIS")
    print_timestamp()
    
    print_section("Data Loading")
    print(f"[ INFO ] Input file: {CSV}")
    print(f"[ INFO ] Conflict threshold: {THRESH} cycles")
    print(f"[ INFO ] Subsample size: {SUBSAMPLE}")
    print(f"[ INFO ] Repeat count: {REPEAT}")
    if LIMIT:
        print(f"[ WARN ] Processing limit: {LIMIT:,} pairs")
    
    try:
        print(f"\n[INFO] Loading data from {CSV}...")
        chunks = []
        chunk_size = 1000000  # Adjust based on available memory
        if LIMIT:
            chunk_size = min(chunk_size, LIMIT)
        
        total_rows = 0
        chunk_count = 0
        
        # Read and process CSV in chunks
        for chunk in pd.read_csv(
            CSV,
            usecols=["a1", "a2", "elapsed_cycles"],
            dtype={"a1": str, "a2": str, "elapsed_cycles": np.uint32},
            chunksize=chunk_size
        ):
            chunk_count += 1
            chunk_rows = len(chunk)
            total_rows += chunk_rows
            print(f"   Chunk {chunk_count}: {chunk_rows:,} rows loaded")
            
            # Process each chunk
            chunk["conflict"] = chunk["elapsed_cycles"] > THRESH
            chunk_a1 = chunk["a1"].apply(lambda x: int(x, 16)).to_numpy(np.uint64)
            chunk_a2 = chunk["a2"].apply(lambda x: int(x, 16)).to_numpy(np.uint64)
            chunk_conf = chunk["conflict"].to_numpy(np.uint8)
            chunk_cycles = chunk["elapsed_cycles"].to_numpy(np.uint32)
            chunks.append((chunk_a1, chunk_a2, chunk_conf, chunk_cycles))
            
            # Show timing distribution for first chunk
            if len(chunks) == 1:
                print("\n[STATS] Generating timing distribution histogram...")
                plt.figure(figsize=(10, 6))
                plt.hist(chunk["elapsed_cycles"], bins=np.linspace(0, chunk["elapsed_cycles"].max(), chunk["elapsed_cycles"].max()), 
                        alpha=0.7, label='Elapsed Cycles', density=True, color='skyblue')
                plt.axvline(x=THRESH, color='red', linestyle='--', linewidth=2, 
                           label=f'Conflict Threshold ({THRESH})')
                plt.xlabel('Elapsed Cycles')
                plt.ylabel('Density')
                plt.title('Memory Access Timing Distribution')
                plt.legend()
                plt.grid(True, alpha=0.3)
                plt.tight_layout()
                plt.show()
            
            if LIMIT and total_rows >= LIMIT:
                print(f"[SUCCESS] Reached processing limit of {LIMIT:,} rows")
                break
            if len(chunks) > 10:  # Safety limit
                print("[WARN]  Reached chunk limit (10 chunks)")
                break
        
        print(f"\n[SUCCESS] Data loading complete!")
        print(f"   Total rows loaded: {total_rows:,}")
        print(f"   Chunks processed: {len(chunks)}")
        
        return chunks
        
    except FileNotFoundError:
        print(f"[ERROR] Error: Input file '{CSV}' not found")
        sys.exit(1)
    except Exception as e:
        print(f"[ERROR] Error loading data: {e}")
        sys.exit(1)

chunks = load_and_preprocess_data()
print_section("Data Preprocessing and Filtering")

# Combine all chunks
print("[INFO] Combining data chunks...")
a1 = np.concatenate([c[0] for c in chunks])
a2 = np.concatenate([c[1] for c in chunks])
conf = np.concatenate([c[2] for c in chunks])
cycles = np.concatenate([c[3] for c in chunks])

print(f"   Combined dataset: {len(a1):,} address pairs")

# Create DataFrame for consistency filtering
print("[INFO] Creating consistency filter...")
pairs_df = pd.DataFrame({
    "a1": a1, 
    "a2": a2, 
    "conf": conf.astype(np.uint8), 
    "cycles": cycles.astype(np.uint32)
})

# Filter for consistent conflict behavior
print("[INFO] Filtering inconsistent pairs...")
print("   (Removing pairs where same addresses show both conflict and non-conflict)")

# Detect groups whose 'conf' column is not unique
flag = pairs_df.groupby(["a1", "a2"])["conf"].nunique().eq(1)

# Keep only groups with a single unique value of 'conf'
pairs_df = pairs_df.merge(
    flag.rename("keep").reset_index(),
    on=["a1", "a2"],
    how="inner"
).query("keep").drop(columns="keep")

original_count = len(conf)
filtered_count = len(pairs_df)
dropped_count = original_count - filtered_count

print(f"[SUCCESS] Consistency filtering complete:")
print(f"   Rows kept: {filtered_count:,}")
print(f"   Rows dropped: {dropped_count:,} ({dropped_count/original_count*100:.1f}%)")

# Update arrays with consistent pairs only
a1 = pairs_df['a1'].values
a2 = pairs_df['a2'].values
conf = pairs_df['conf'].values
cycles = pairs_df['cycles'].values
N = len(a1)

print_section("Dataset Statistics")
conflict_count = np.sum(conf)
conflict_rate = np.mean(conf) * 100

print(f"[STATS] Final dataset statistics:")
print(f"   Total pairs: {N:,}")
print(f"   Conflict pairs: {conflict_count:,}")
print(f"   Non-conflict pairs: {N - conflict_count:,}")
print(f"   Conflict rate: {conflict_rate:.2f}%")

# Show timing statistics
conflict_cycles = cycles[conf == 1]
non_conflict_cycles = cycles[conf == 0]

print(f"\n[STATS] Timing statistics:")
print(f"   Conflict cycles - mean: {np.mean(conflict_cycles):.1f}, std: {np.std(conflict_cycles):.1f}")
print(f"   Non-conflict cycles - mean: {np.mean(non_conflict_cycles):.1f}, std: {np.std(non_conflict_cycles):.1f}")
print(f"   Overall - mean: {np.mean(cycles):.1f}, std: {np.std(cycles):.1f}")

if conflict_count == 0:
    print("[ERROR] Error: No conflict pairs found! Check your threshold value.")
    sys.exit(1)
if N - conflict_count == 0:
    print("[ERROR] Error: No non-conflict pairs found! Check your threshold value.")
    sys.exit(1)

# ════════════════════════════════════════════════════════════════════════════════
# Binary Difference Matrix Construction
# ════════════════════════════════════════════════════════════════════════════════

print_section("Building Binary Difference Matrix")

print("[INFO] Computing XOR differences between address pairs...")
# XOR of the physical addresses
diffs_int = np.bitwise_xor(a1, a2)  # shape (N,)

print("[INFO] Converting to bit-level representation...")
# Expand every 64-bit word to 64 single-bit columns
diffs_bits = np.unpackbits(
    diffs_int.view(np.uint8).reshape(-1, 8),  # (N,8) bytes
    axis=1,
    bitorder="little"
)[:, :64].astype(np.uint8)  # (N,64) bits

# Keep only columns that are ever 1 (remove always-zero columns)
nonzero_cols = np.flatnonzero(diffs_bits.any(axis=0))
diffs_bits = diffs_bits[:, nonzero_cols]

print(f"[SUCCESS] Binary difference matrix constructed:")
print(f"   Matrix shape: {diffs_bits.shape[0]:,} × {diffs_bits.shape[1]}")
print(f"   Non-zero columns: {len(nonzero_cols)} / 64")
print(f"   Bit positions with differences: {list(nonzero_cols)}")

# Pre-compute full bit-decompositions of a1/a2 for later evaluation
print("[INFO] Pre-computing bit decompositions for addresses...")
a1_bits = np.unpackbits(a1.view(np.uint8).reshape(-1, 8),
                        axis=1, bitorder="little")[:, :64].astype(np.uint8)
a2_bits = np.unpackbits(a2.view(np.uint8).reshape(-1, 8),
                        axis=1, bitorder="little")[:, :64].astype(np.uint8)

print("[SUCCESS] Bit decompositions ready for analysis")

# ════════════════════════════════════════════════════════════════════════════════
# GF(2) Nullspace Analysis for Bank Mask Discovery
# ════════════════════════════════════════════════════════════════════════════════

print_section("GF(2) Nullspace Analysis")

# Initialize Galois field
GF = galois.GF(2)

def gf2_rank(A):
    """Rank of a GF(2) matrix, compatible with all galois versions."""
    return np.linalg.matrix_rank(A)

mask_counter = Counter()

print(f"[INFO] Starting subsampling analysis...")
print(f"   Subsample size: {SUBSAMPLE}")
print(f"   Number of repetitions: {REPEAT}")

# We ONLY want rows that are *conflicts* (equation m·d = 0)
rows_conflict = np.nonzero(conf == 1)[0]
conflict_count = len(rows_conflict)

print(f"   Conflict rows available: {conflict_count:,}")

if conflict_count == 0:
    print("[ERROR] Error: No conflict rows found → nothing to solve!")
    sys.exit(1)

if conflict_count < SUBSAMPLE:
    print(f"[WARN]  Warning: Only {conflict_count} conflict rows available, less than subsample size {SUBSAMPLE}")
    actual_subsample = conflict_count
else:
    actual_subsample = SUBSAMPLE

print(f"   Actual subsample size: {actual_subsample}")
print()

# Progress tracking
successful_rounds = 0
total_nullspace_vectors = 0

for r in range(REPEAT):
    print(f"   Round {r+1:3d}/{REPEAT}: ", end="", flush=True)
    
    try:
        # Draw without replacement; fall back to all if sample size ≥ population
        idx = np.random.choice(rows_conflict,
                              size=min(actual_subsample, len(rows_conflict)),
                              replace=False)
        D_sub = diffs_bits[idx]  # binary matrix (s, K)
        
        D_gf2 = GF(D_sub)  # convert to GF(2)
        nullspace = D_gf2.null_space()  # shape (nullity, K)
        
        rank = D_sub.shape[1] - nullspace.shape[0]
        nullity = nullspace.shape[0]
        
        print(f"rank: {rank:2d}, nullspace dim: {nullity:2d}, vectors found: {len(nullspace)}")
        
        # Store every mask from the null-space
        for vec in nullspace:
            # vec has only the non-zero columns; expand back to 64 bits
            mask = np.zeros(64, dtype=np.uint8)
            mask[nonzero_cols] = vec
            mask_counter[tuple(mask)] += 1
            total_nullspace_vectors += 1
        
        successful_rounds += 1
        
    except Exception as e:
        print(f"[ERROR] Error: {e}")
        continue

print(f"\n[SUCCESS] Subsampling analysis complete:")
print(f"   Successful rounds: {successful_rounds}/{REPEAT}")
print(f"   Total nullspace vectors found: {total_nullspace_vectors}")
print(f"   Unique masks discovered: {len(mask_counter)}")

if len(mask_counter) == 0:
    print("[ERROR] Error: No masks found! Try adjusting the threshold or increasing subsample size.")
    sys.exit(1)

# ════════════════════════════════════════════════════════════════════════════════
# Mask Collection and Linear Independence Analysis
# ════════════════════════════════════════════════════════════════════════════════

print_section("Mask Analysis and Optimization")

# Collect ALL distinct masks the solver produced
found_masks = [
    np.array(k, dtype=np.uint8)
    for k, _ in mask_counter.most_common()  # sorted by frequency
]

k = len(found_masks)
print(f"[SEARCH] Found {k} distinct candidate masks:")

if VERBOSE:
    for i, m in enumerate(found_masks[:10]):  # Show first 10 in verbose mode
        m_val = np.packbits(m, bitorder="little").view(np.uint64)[0]
        frequency = mask_counter[tuple(m)]
        set_bits = [j for j, b in enumerate(m) if b == 1]
        print(f"   {i:2d}: {m_val:#018x} (seen {frequency:3d}×, weight: {len(set_bits)}, bits: {set_bits})")
    if k > 10:
        print(f"   ... and {k-10} more masks")
else:
    # Show summary in non-verbose mode
    frequencies = [mask_counter[tuple(m)] for m in found_masks]
    weights = [np.sum(m) for m in found_masks]
    print(f"   Frequency range: {min(frequencies)} - {max(frequencies)}")
    print(f"   Weight range: {min(weights)} - {max(weights)} bits")

def _weight(mask):
    """Hamming-weight that accepts int or 0/1 NumPy array."""
    if isinstance(mask, np.ndarray):
        return int(mask.sum())
    else:  # assume Python int
        return mask.bit_count()

def _as_bits(mask):
    """Return mask as list[0|1] length-64, regardless of representation."""
    if isinstance(mask, np.ndarray):
        return mask.tolist()
    else:  # integer
        return [(mask >> i) & 1 for i in range(64)]

def remove_linear_combinations(masks, target_dimension=None):
    """
    Keep the lightest linearly-independent masks (GF(2) rank test).

    Parameters
    ----------
    masks : list[int | np.ndarray]
        Row-mask candidates as 64-bit ints *or* 64-element 0/1 arrays.
    target_dimension : int | None
        Optional – stop once this many independent masks are kept.

    Returns
    -------
    kept : list[int | np.ndarray]
        Same representation as input, but only independent, lightest first.
    """
    # 1) sort once by Hamming weight
    order = sorted(range(len(masks)), key=lambda idx: _weight(masks[idx]))

    def to_int(mask):
        if isinstance(mask, np.ndarray):
            return int(np.packbits(mask, bitorder="little").view(np.uint64)[0])
        return int(mask)

    def to_array(mask_int):
        raw = np.array([mask_int], dtype=np.uint64).view(np.uint8)
        return np.unpackbits(raw, bitorder="little")[:64].astype(np.uint8)

    basis = [0] * 64
    kept_ints = []

    for idx in order:
        value = to_int(masks[idx])
        candidate = value
        independent = False
        for bit in range(63, -1, -1):
            pivot = 1 << bit
            if candidate & pivot == 0:
                continue
            if basis[bit] == 0:
                basis[bit] = candidate
                independent = True
                break
            candidate ^= basis[bit]

        if independent:
            kept_ints.append(value)
            if target_dimension and len(kept_ints) >= target_dimension:
                break

    if isinstance(masks[0], np.ndarray):
        return [to_array(value) for value in kept_ints]
    return kept_ints


def minimal_hamming_weight_basis(masks):
    """
    Return the lowest‑weight linearly‑independent subset of `masks`.
    Each mask is a length‑64 0/1 NumPy array.
    """
    # Sort once by individual Hamming weight
    masks_sorted = sorted(masks, key=lambda m: int(m.sum()))

    # Dimension of the subspace spanned by ALL candidates
    full_rank = gf2_rank(GF(np.stack(masks_sorted)))

    best_basis   = None
    best_weight  = np.inf

    # Enumerate all combinations of that size
    for combo in itertools.combinations(masks_sorted, full_rank):
        A = GF(np.stack(combo))
        if gf2_rank(A) == full_rank:         # still a basis
            w = sum(int(v.sum()) for v in combo)
            if w < best_weight:
                best_basis, best_weight = combo, w

    return list(best_basis), best_weight

print("[INFO] Finding optimal minimal-weight basis...")
best_basis, min_weight = minimal_hamming_weight_basis(found_masks)

print(f"\n[STATS] Optimal minimal-weight basis ({len(best_basis)} masks):")
for i, mask in enumerate(best_basis):
    m_val = np.packbits(mask, bitorder="little").view(np.uint64)[0]
    bits_set = [j for j, b in enumerate(mask) if b]
    print(f"   {i:2d}: {m_val:#018x}, weight: {len(bits_set)}, bits: {bits_set}")

print(f"   Total minimal Hamming weight: {min_weight}")
found_masks = best_basis
# Remove linear combinations to get independent set
print("\n[INFO] Removing linear combinations...")
found_masks = remove_linear_combinations(found_masks)
k = len(found_masks)

print(f"[SUCCESS] Linear independence analysis complete:")
print(f"   Independent masks kept: {k}")

if VERBOSE:
    print(f"\n[LIST] Final independent mask set:")
    for i, m in enumerate(found_masks):
        m_val = np.packbits(m, bitorder="little").view(np.uint64)[0]
        frequency = mask_counter[tuple(m)]
        set_bits = [j for j, b in enumerate(m) if b == 1]
        print(f"   {i:2d}: {m_val:#018x} (seen {frequency:3d}×, weight: {len(set_bits)}, bits: {set_bits})")

# ════════════════════════════════════════════════════════════════════════════════
# Accuracy Evaluation and Performance Metrics
# ════════════════════════════════════════════════════════════════════════════════

print_section("Performance Evaluation")

# Pre-compute parity differences for every mask
print("[INFO] Computing parity predictions for all masks...")
pred_matrix = np.empty((k, N), dtype=np.uint8)  # 0 = match, 1 = differ

for i, m in enumerate(found_masks):
    sel_a = a1_bits[:, m == 1]
    sel_b = a2_bits[:, m == 1]
    pred_matrix[i] = parity(sel_a ^ sel_b)

print("[SUCCESS] Parity predictions computed")

# Score each mask individually
print("\n[STATS] Individual mask performance:")
print("     Mask                   | Error  | False Neg | False Pos")
print("   " + "-" * 55)

individual_scores = []
for i in range(k):
    pdiff = pred_matrix[i]  # parity diff for mask i
    fn = ((conf == 1) & (pdiff == 1)).sum()  # predicted miss, real hit
    fp = ((conf == 0) & (pdiff == 0)).sum()  # predicted hit, real miss
    err = (fn + fp) / N * 100
    fnr = fn / max(1, (conf == 1).sum()) * 100
    fpr = fp / max(1, (conf == 0).sum()) * 100
    m_val = np.packbits(found_masks[i], bitorder="little").view(np.uint64)[0]
    
    individual_scores.append((err, fnr, fpr))
    print(f"   {i:2d}: {m_val:#016x} | {err:5.2f}% | {fnr:8.2f}% | {fpr:8.2f}%")

# Score the conjunction of *all* masks
print_section("Combined Mask Performance")
print("[INFO] Evaluating conjunction of all masks...")

pred_conflict = (pred_matrix == 0).all(axis=0)  # hit if every mask matches
real_conflict = conf == 1

# Calculate confusion matrix
true_pos = np.count_nonzero(real_conflict & pred_conflict)
true_neg = np.count_nonzero(~real_conflict & ~pred_conflict)
false_pos = np.count_nonzero(~real_conflict & pred_conflict)
false_neg = np.count_nonzero(real_conflict & ~pred_conflict)

# Calculate metrics
total = true_pos + true_neg + false_pos + false_neg
accuracy = (true_pos + true_neg) / total * 100
precision = true_pos / max(1, true_pos + false_pos) * 100
recall = true_pos / max(1, true_pos + false_neg) * 100
f1_score = 2 * (precision * recall) / max(1, precision + recall)

print("[SUCCESS] Combined performance evaluation:")
print(f"\n[STATS] Confusion Matrix:")
print(f"                    Predicted")
print(f"                 Conflict | No Conflict")
print(f"   Actual Conflict    {true_pos:6,} | {false_neg:9,}")
print(f"   Actual No Conflict {false_pos:6,} | {true_neg:9,}")

print(f"\n[ANALYSIS] Performance Metrics:")
print(f"   Accuracy:  {accuracy:.4f}%")
print(f"   Precision: {precision:.4f}%")
print(f"   Recall:    {recall:.4f}%")
print(f"   F1-Score:  {f1_score:.4f}")

print(f"\n[LIST] Detailed Counts:")
print(f"   True Positives (TP):  {true_pos:,}")
print(f"   True Negatives (TN):  {true_neg:,}")
print(f"   False Positives (FP): {false_pos:,}")
print(f"   False Negatives (FN): {false_neg:,}")
print(f"   Total Samples:        {total:,}")

def get_set_bit_indices(mask):
    """Return list of indices where bits are set to 1."""
    return [i for i, bit in enumerate(mask) if bit == 1]

def display_bank_masks(masks, title="Bank Masks"):
    """Display bank masks with enhanced formatting showing set bit indices."""
    print_header(title)
    
    if not masks:
        print("[ERROR] No masks found.")
        return
    
    for i, mask in enumerate(masks):
        # Convert to hex value
        m_val = np.packbits(mask, bitorder="little").view(np.uint64)[0]
        
        # Get set bit indices
        set_bits = get_set_bit_indices(mask)
        
        # Calculate Hamming weight
        hamming_weight = len(set_bits)
        
        # Format binary representation (show first and last 16 bits)
        binary_repr = ''.join(str(b) for b in mask)
        if len(binary_repr) > 32:
            binary_display = f"{binary_repr[:16]}...{binary_repr[-16:]}"
        else:
            binary_display = binary_repr
        
        print(f"[RESULT] Bank Mask {i:2d}:")
        print(f"   Hex Value:   {m_val:#018x}")
        print(f"   Weight:      {hamming_weight:2d} bits")
        print(f"   Set Bits:    {set_bits}")
        print(f"   Binary:      {binary_display}")
        print()

display_bank_masks(found_masks, title="Final Bank Conflict Masks")

# ════════════════════════════════════════════════════════════════════════════════
# Bank-Separated Timing Analysis
# ════════════════════════════════════════════════════════════════════════════════

print_section("Bank-Separated Timing Distribution")

print("[INFO] Analyzing timing patterns by predicted bank grouping...")

same_bank_cycles = []
diff_bank_cycles = []

# Use the same prediction matrix as before
for i in range(N):
    if pred_conflict[i]:  # Same bank (all masks predict conflict)
        same_bank_cycles.append(cycles[i])
    else:  # Different bank
        diff_bank_cycles.append(cycles[i])

print(f"[STATS] Bank separation results:")
print(f"   Same bank pairs:      {len(same_bank_cycles):,} ({len(same_bank_cycles)/N*100:.1f}%)")
print(f"   Different bank pairs: {len(diff_bank_cycles):,} ({len(diff_bank_cycles)/N*100:.1f}%)")

# Analyze timing within each group
same_bank_low = [c for c in same_bank_cycles if c <= THRESH]
same_bank_high = [c for c in same_bank_cycles if c > THRESH]
diff_bank_low = [c for c in diff_bank_cycles if c <= THRESH]
diff_bank_high = [c for c in diff_bank_cycles if c > THRESH]

print(f"\n[ANALYSIS] Detailed timing statistics:")
print(f"   Same Bank Pairs ({len(same_bank_cycles):,} total):")
print(f"     Low latency (≤{THRESH}):  {len(same_bank_low):,} ({len(same_bank_low)/len(same_bank_cycles)*100:.1f}%)")
print(f"     High latency (>{THRESH}): {len(same_bank_high):,} ({len(same_bank_high)/len(same_bank_cycles)*100:.1f}%)")

print(f"   Different Bank Pairs ({len(diff_bank_cycles):,} total):")
print(f"     Low latency (≤{THRESH}):  {len(diff_bank_low):,} ({len(diff_bank_low)/len(diff_bank_cycles)*100:.1f}%)")
print(f"     High latency (>{THRESH}): {len(diff_bank_high):,} ({len(diff_bank_high)/len(diff_bank_cycles)*100:.1f}%)")

if same_bank_cycles and diff_bank_cycles:
    print(f"\n[STATS] Timing comparison:")
    print(f"   Same bank - mean: {np.mean(same_bank_cycles):.1f}, std: {np.std(same_bank_cycles):.1f}")
    print(f"   Diff bank - mean: {np.mean(diff_bank_cycles):.1f}, std: {np.std(diff_bank_cycles):.1f}")

# ════════════════════════════════════════════════════════════════════════════════
# Row Buffer Analysis
# ════════════════════════════════════════════════════════════════════════════════

print_section("Row Buffer Analysis (Strict Invariant Method)")

print("[INFO] Analyzing row buffer behavior patterns...")

# Convert bank masks to integers
bank_int = [
    int(np.packbits(m, bitorder="little").view(np.uint64)[0])
    if isinstance(m, np.ndarray) else int(m)
    for m in found_masks
]

print(f"   Bank masks: {len(bank_int)} patterns identified")

# Analyze same-bank pairs only
SB_idx = np.where(pred_conflict == 1)[0]
if len(SB_idx) == 0:
    print("[WARN]  Warning: No same-bank pairs found for row analysis")
else:
    SB_a1, SB_a2, SB_cycles = a1_bits[SB_idx], a2_bits[SB_idx], cycles[SB_idx]
    hits = SB_cycles <= THRESH
    conflicts = SB_cycles > THRESH
    
    print(f"   Same-bank pairs: {len(SB_idx):,}")
    print(f"   Row hits: {hits.sum():,} ({hits.sum()/len(SB_idx)*100:.1f}%)")
    print(f"   Row conflicts: {conflicts.sum():,} ({conflicts.sum()/len(SB_idx)*100:.1f}%)")

    if hits.sum() == 0 or conflicts.sum() == 0:
        print("[WARN]  Warning: Need both hit and conflict pairs for row analysis")
        print("   Consider adjusting the threshold value")
    else:
        # Analyze bit behavior patterns
        observed_mask = np.zeros(64, bool)
        observed_mask[nonzero_cols] = True
        vary_mask = (SB_a1[hits] ^ SB_a2[hits]).any(axis=0)
        invariant_mask = ~vary_mask
        conflict_mask = (SB_a1[conflicts] ^ SB_a2[conflicts]).any(axis=0)

        print(f"   Observed bit positions: {np.sum(observed_mask)} / 64")
        print(f"   Invariant bit positions: {np.sum(invariant_mask)} / 64")
        print(f"   Conflict-associated bits: {np.sum(conflict_mask)} / 64")

        # Generate nullspace for row analysis
        M_bin = np.array([[((b >> i) & 1) for i in range(64)]
                         for b in bank_int], dtype=np.uint8)
        GF2 = galois.GF(2)
        
        try:
            nullspace = GF2(M_bin).null_space()
            row_vecs = [
                sum(1 << i for i, b in enumerate(v) if b)
                for v in nullspace
            ]
            
            print(f"   Initial row candidates: {len(row_vecs)}")

            # Apply strict filter
            def good(v: int) -> bool:
                bits = [i for i in range(64) if (v >> i) & 1]
                return (bits and
                       all(observed_mask[bits]) and
                       all(invariant_mask[bits]) and
                       any(conflict_mask[bits]))

            row_vecs = [v for v in row_vecs if good(v)]
            print(f"   After strict filter: {len(row_vecs)} candidates")

            if row_vecs:
                # Find independent basis
                def wt(x): return x.bit_count()
                
                B = GF2(M_bin.copy())
                row_basis = []
                for v in sorted(row_vecs, key=wt):
                    vec = GF2([(v >> i) & 1 for i in range(64)])
                    if np.linalg.matrix_rank(GF2(np.vstack([B, vec]))) > B.shape[0]:
                        row_basis.append(v)
                        B = GF2(np.vstack([B, vec]))

                print(f"\n[RESULT] Row buffer masks identified ({len(row_basis)} patterns):")
                for i, v in enumerate(row_basis):
                    bits = [b for b in range(64) if (v >> b) & 1]
                    print(f"   Row {i:2d}: {v:#018x} (weight: {wt(v)}, bits: {bits})")

                # Verify rank
                expected_rank = M_bin.shape[0] + len(row_basis)
                actual_rank = np.linalg.matrix_rank(B)
                rank_check = "[SUCCESS]" if actual_rank == expected_rank else "[ERROR]"
                print(f"\n[STATS] Rank verification: {actual_rank}/{expected_rank} {rank_check}")
                
            else:
                print("[ERROR] No valid row candidates found - may need more data or different threshold")
                
        except Exception as e:
            print(f"[ERROR] Error in row analysis: {e}")

# ════════════════════════════════════════════════════════════════════════════════
# Analysis Summary and Conclusion
# ════════════════════════════════════════════════════════════════════════════════

print_header("ANALYSIS SUMMARY")

print("[RESULT] Key Findings:")
print(f"   • Dataset: {N:,} address pairs analyzed")
print(f"   • Conflict threshold: {THRESH} cycles") 
print(f"   • Bank masks discovered: {len(found_masks)}")
print(f"   • Overall accuracy: {accuracy:.2f}%")
print(f"   • Precision: {precision:.2f}%")
print(f"   • Recall: {recall:.2f}%")

if len(found_masks) > 0:
    total_bits = sum(np.sum(m) for m in found_masks)
    print(f"   • Total mask complexity: {total_bits} bits across {len(found_masks)} masks")
    
    # Show the most significant mask
    best_mask = found_masks[0]  # First mask (highest frequency)
    best_mask_val = np.packbits(best_mask, bitorder="little").view(np.uint64)[0]
    best_mask_bits = [i for i, b in enumerate(best_mask) if b == 1]
    print(f"   • Primary mask: {best_mask_val:#018x} (bits: {best_mask_bits})")

print(f"\n[FILE] Analysis completed successfully!")
print(f"   Input file: {CSV}")
print(f"   Analysis time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if accuracy > 90:
    print("[EXCELLENT] Excellent accuracy achieved! The discovered masks provide strong conflict prediction.")
elif accuracy > 75:
    print("[SUCCESS] Good accuracy achieved. The masks show clear memory organization patterns.")
elif accuracy > 60:
    print("[WARN]  Moderate accuracy. Consider adjusting threshold or collecting more data.")
else:
    print("[ERROR] Low accuracy. Check threshold value and data quality.")

print("\n" + "="*80)
